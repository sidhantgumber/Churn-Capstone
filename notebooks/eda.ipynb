{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/customer_intelligence_dataset.csv\")\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "display(df.head(10))\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(df.duplicated().sum())\n",
    "print(\"\\nDescription:\", df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fae961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "df = df.drop(columns=[\"total_value\"])\n",
    "df['total_value'] = df['price'] * df['quantity']\n",
    "df[\"total_value_log\"] = np.log1p(df[\"total_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47696ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"sale_date\", \"last_purchase_date\"]:\n",
    "    df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"sale_year\"]    = df[\"sale_date\"].dt.year.astype(\"Int16\")\n",
    "df[\"sale_month\"]   = df[\"sale_date\"].dt.month.astype(\"Int8\")\n",
    "df[\"sale_day\"]     = df[\"sale_date\"].dt.day.astype(\"Int8\")\n",
    "df[\"sale_dow\"]     = df[\"sale_date\"].dt.dayofweek.astype(\"Int8\")  # 0=Mon ... 6=Sun\n",
    "df[\"is_weekend\"]   = (df[\"sale_dow\"] >= 5).astype(\"uint8\")    \n",
    "\n",
    "for c in [\"sale_id\", \"customer_id\", \"product_id\", \"product_name\"]:\n",
    "    df[c] = df[c].astype(\"string\")\n",
    "\n",
    "for c in [\"gender\", \"region\", \"segment\", \"category\", \"sentiment\"]:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b47513",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df[\"churn\"].unique()) <= {0, 1}, f\"Unexpected churn values: {df['churn'].unique()}\"\n",
    "df[\"churn\"] = df[\"churn\"].astype(\"uint8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"\\nNumeric columns:\", num_cols)\n",
    "display(df[num_cols].describe().T if num_cols else \"No numeric columns found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"\\nCategorical columns:\", cat_cols)\n",
    "for c in cat_cols[:5]:\n",
    "    print(f\"\\nTop 10 values for '{c}':\")\n",
    "    print(df[c].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_customers = df.groupby('customer_id')['total_value'].sum().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 highest paying customers:\")\n",
    "display(top_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in num_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[c].dropna(), bins=30, kde=False)\n",
    "    plt.title(f\"Distribution: {c}\")\n",
    "    plt.xlabel(c); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_cols:\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.boxplot(data=df, x=col, orient=\"h\")\n",
    "        plt.title(f\"Boxplot: {col}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_cols) >= 2:\n",
    "    corr = df[num_cols].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "    plt.title(\"Correlation Heatmap (numeric)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "df_fe = df.copy()\n",
    "\n",
    "assert pd.api.types.is_datetime64_any_dtype(df_fe[\"sale_date\"]), \"sale_date must be datetime\"\n",
    "assert \"total_value\" in df_fe.columns, \"total_value missing\"\n",
    "assert \"total_value_log\" in df_fe.columns, \"total_value_log missing\"\n",
    "assert \"is_weekend\" in df_fe.columns and \"sale_dow\" in df_fe.columns, \"derive date features first\"\n",
    "\n",
    "AS_OF_DATE = df_fe[\"sale_date\"].max()\n",
    "HIGH_TICKET_THR = df_fe[\"total_value\"].quantile(0.90)\n",
    "\n",
    "df_fe[\"feedback_text\"] = df_fe.get(\"feedback_text\", \"\").fillna(\"\").astype(str).str.strip()\n",
    "df_fe[\"has_feedback\"] = (df_fe[\"feedback_text\"].str.len() > 0).astype(\"uint8\")\n",
    "\n",
    "try:\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    _ = nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def _score_text(t: str) -> float:\n",
    "    return float(sia.polarity_scores(t)[\"compound\"]) if t else 0.0\n",
    "\n",
    "df_fe[\"sentiment_score\"] = df_fe[\"feedback_text\"].map(_score_text)\n",
    "\n",
    "def _label_from_score(s: float) -> str:\n",
    "    if s >= 0.30:   return \"Positive\"\n",
    "    if s <= -0.30:  return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "df_fe[\"sentiment_pred\"] = df_fe[\"sentiment_score\"].map(_label_from_score).astype(\"category\")\n",
    "\n",
    "sent_map = {\"positive\":\"Positive\", \"negative\":\"Negative\", \"neutral\":\"Neutral\"}\n",
    "df_fe[\"sentiment_norm\"] = np.where(\n",
    "    df_fe[\"has_feedback\"].eq(1),\n",
    "    df_fe[\"sentiment_pred\"].astype(str),\n",
    "    np.nan,\n",
    ")\n",
    "if \"sentiment\" in df_fe.columns:\n",
    "    df_fe.loc[df_fe[\"sentiment_norm\"].isna(), \"sentiment_norm\"] = (\n",
    "        df_fe[\"sentiment\"].astype(str).str.lower().map(sent_map)\n",
    "    )\n",
    "df_fe[\"sentiment_norm\"] = df_fe[\"sentiment_norm\"].fillna(\"NoFeedback\").astype(\"category\")\n",
    "\n",
    "df_fe[\"is_high_ticket\"] = (df_fe[\"total_value\"] > HIGH_TICKET_THR).astype(\"uint8\")\n",
    "\n",
    "def build_customer_features(g: pd.DataFrame) -> pd.Series:\n",
    "    first_sale = g[\"sale_date\"].min()\n",
    "    last_sale  = g[\"sale_date\"].max()\n",
    "    orders     = int(len(g))\n",
    "\n",
    "    m_sum   = float(g[\"total_value\"].sum())\n",
    "    m_med   = float(g[\"total_value\"].median())\n",
    "    m_max   = float(g[\"total_value\"].max())\n",
    "    q_mean  = float(g[\"quantity\"].mean())\n",
    "    q_max   = float(g[\"quantity\"].max())\n",
    "    p_mean  = float(g[\"price\"].mean())\n",
    "\n",
    "    dow_dist = g[\"sale_dow\"].value_counts(normalize=True).reindex(range(7), fill_value=0.0)\n",
    "\n",
    "    uniq_cat = int(g[\"category\"].nunique())\n",
    "    uniq_prod = int(g[\"product_id\"].nunique())\n",
    "    vc_cat = g[\"category\"].value_counts(normalize=True)\n",
    "    top_cat = vc_cat.index[0] if len(vc_cat) else np.nan\n",
    "    top_cat_share = float(vc_cat.iloc[0]) if len(vc_cat) else 0.0\n",
    "\n",
    "    with_fb = g[g[\"has_feedback\"] == 1]\n",
    "    pos = int((with_fb[\"sentiment_norm\"] == \"Positive\").sum())\n",
    "    neg = int((with_fb[\"sentiment_norm\"] == \"Negative\").sum())\n",
    "    neu = int((with_fb[\"sentiment_norm\"] == \"Neutral\").sum())\n",
    "    den = max(1, pos + neg + neu)\n",
    "    neg_rate = float(neg / den)\n",
    "\n",
    "    last_sent = g.sort_values(\"sale_date\")[\"sentiment_norm\"].iloc[-1] if len(g) else \"NoFeedback\"\n",
    "\n",
    "    recent_mask = g[\"sale_date\"] >= (AS_OF_DATE - pd.Timedelta(days=90))\n",
    "    recent_neg_flag = int(((g.loc[recent_mask & (g[\"has_feedback\"] == 1), \"sentiment_norm\"] == \"Negative\").sum()) > 0)\n",
    "\n",
    "    feedback_count = int((g[\"has_feedback\"] == 1).sum())\n",
    "    feedback_rate = float(feedback_count / max(1, len(g)))\n",
    "\n",
    "    g_sorted = g.sort_values(\"sale_date\")\n",
    "    age_latest = int(g_sorted[\"age\"].iloc[-1])\n",
    "    gender_mode = g[\"gender\"].mode(dropna=False)\n",
    "    gender_final = gender_mode.iloc[0] if not gender_mode.empty else np.nan\n",
    "    region_mode = g[\"region\"].mode(dropna=False)\n",
    "    region_final = region_mode.iloc[0] if not region_mode.empty else np.nan\n",
    "\n",
    "    tenure_days = int((last_sale - first_sale).days)\n",
    "    recency_days = int((AS_OF_DATE - last_sale).days)\n",
    "\n",
    "    high_ticket_rate = float(g[\"is_high_ticket\"].mean())\n",
    "\n",
    "    return pd.Series({\n",
    "        \"orders\": orders,\n",
    "        \"first_sale\": first_sale,\n",
    "        \"last_sale\": last_sale,\n",
    "        \"tenure_days\": tenure_days,\n",
    "        \"recency_days\": recency_days,\n",
    "\n",
    "        \"monetary_sum\": m_sum,\n",
    "        \"monetary_median\": m_med,\n",
    "        \"monetary_max\": m_max,\n",
    "\n",
    "        \"avg_quantity\": q_mean,\n",
    "        \"max_quantity\": q_max,\n",
    "        \"avg_price\": p_mean,\n",
    "\n",
    "        \"dow_0_rate\": float(dow_dist.loc[0]),\n",
    "        \"dow_1_rate\": float(dow_dist.loc[1]),\n",
    "        \"dow_2_rate\": float(dow_dist.loc[2]),\n",
    "        \"dow_3_rate\": float(dow_dist.loc[3]),\n",
    "        \"dow_4_rate\": float(dow_dist.loc[4]),\n",
    "        \"dow_5_rate\": float(dow_dist.loc[5]),\n",
    "        \"dow_6_rate\": float(dow_dist.loc[6]),\n",
    "\n",
    "        \"unique_categories\": uniq_cat,\n",
    "        \"unique_products\": uniq_prod,\n",
    "        \"top_category\": top_cat,\n",
    "        \"top_category_share\": top_cat_share,\n",
    "\n",
    "        \"sent_pos\": pos,\n",
    "        \"sent_neg\": neg,\n",
    "        \"sent_neu\": neu,\n",
    "        \"neg_rate\": neg_rate,\n",
    "        \"last_sentiment\": last_sent,\n",
    "        \"recent_neg_flag\": recent_neg_flag,\n",
    "        \"feedback_count\": feedback_count,\n",
    "        \"feedback_rate\": feedback_rate,\n",
    "\n",
    "        \"age_latest\": age_latest,\n",
    "        \"gender\": gender_final,\n",
    "        \"region\": region_final,\n",
    "\n",
    "        \"high_ticket_rate\": high_ticket_rate,\n",
    "    })\n",
    "\n",
    "gb = df_fe.groupby(\"customer_id\", group_keys=False)\n",
    "try:\n",
    "    cust = gb.apply(build_customer_features, include_groups=False).reset_index()\n",
    "except TypeError:\n",
    "    cust = gb.apply(build_customer_features).reset_index()\n",
    "\n",
    "y_cust = df_fe.groupby(\"customer_id\")[\"churn\"].max().rename(\"churn\")\n",
    "cust = cust.merge(y_cust, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "days = cust[\"tenure_days\"].clip(lower=1)\n",
    "cust[\"aov\"] = cust[\"monetary_sum\"] / cust[\"orders\"].clip(lower=1)\n",
    "cust[\"orders_per_30d\"]   = cust[\"orders\"] / (days / 30.0)\n",
    "cust[\"monetary_per_30d\"] = cust[\"monetary_sum\"] / (days / 30.0)\n",
    "\n",
    "cust.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "cust[[\"orders_per_30d\",\"monetary_per_30d\"]] = cust[[\"orders_per_30d\",\"monetary_per_30d\"]].fillna(0)\n",
    "\n",
    "for c in [\"gender\", \"region\", \"top_category\", \"last_sentiment\"]:\n",
    "    cust[c] = cust[c].astype(\"category\")\n",
    "\n",
    "cust_ml = pd.get_dummies(\n",
    "    cust,\n",
    "    columns=[\"gender\", \"region\", \"top_category\", \"last_sentiment\"],\n",
    "    drop_first=True,\n",
    "    dtype=np.uint8\n",
    "    )\n",
    "\n",
    "drop_cols = [\n",
    "    \"first_sale\", \"last_sale\",\n",
    "    \"dow_0_rate\",\n",
    "    \"monetary_mean\"\n",
    "    ]\n",
    "cust_ml.drop(columns=[c for c in drop_cols if c in cust_ml.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(cust_ml.head(10))\n",
    "print(f\"Null vals: {cust_ml.isnull().sum()}\")\n",
    "print(f\"Duplicate vals: {cust_ml.duplicated().sum()}\")\n",
    "\n",
    "X = cust_ml.drop(columns=[\"churn\"], errors=\"ignore\")\n",
    "y = cust_ml[\"churn\"].astype(\"uint8\")\n",
    "print(\"Churn Rate: \", y.mean())\n",
    "\n",
    "print(\"Customers:\", cust_ml.shape[0], \"| Features:\", X.shape[1])\n",
    "print(\"Sample columns:\", list(X.columns)[:20], \"...\")\n",
    "X.corr(numeric_only=True).abs().unstack().sort_values(ascending=False).drop_duplicates().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_drop = [\n",
    "    \"aov\", \"monetary_median\", \"monetary_max\", \"avg_price\",\n",
    "    \"unique_products\", \"max_quantity\", \"unique_categories\"\n",
    " ]\n",
    "X_linear = X.drop(columns=[c for c in linear_drop if c in X.columns])\n",
    "\n",
    "# Outlier capping is commented out below\n",
    "# for c in [\"orders_per_30d\", \"monetary_per_30d\"]:\n",
    "#     p99 = X[c].quantile(0.99)\n",
    "#     X[c] = X[c].clip(upper=p99)\n",
    "#     if \"X_linear\" in globals() and c in X_linear.columns:\n",
    "#         X_linear[c] = X_linear[c].clip(upper=p99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save engineered dataframes for modeling ===\n",
    "cust_ml.to_csv(\"../data/customer_snapshot_ml.csv\", index=False)\n",
    "X_linear.to_csv(\"../data/customer_snapshot_Xlinear.csv\", index=False)\n",
    "print(\"Saved: customer_snapshot_ml.csv and customer_snapshot_Xlinear.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
